karatsuba multiplication
merge sort
big-oh notation
big omega and theta


Strassen's subcubic matrix multiplication algorithm
O(n log n) algorithm for closest pair

Quicksort and partitioning around a pivot
Randomized selection
Deterministic selection
Comparison-based sorting lower bound
Minimum cuts
Random contraction algorithm

Breadth-first search, shortest paths, undirected connectivity
Computing strongly connected components
Depth-first search
Topological sort
Dijkstra's shortest-path algorithm

Heaps
Binary search tree, red-black trees
Hash tables
Universal hashing
Bloom filters

Greedy algorithm for scheduling 
 and optimal caching
Prim's minimum spanning tree algorithm
Kruskal's MST algorithm

Clustering
Union-find
Path compression
Ackermann function
Hopcroft-Ullman Analysis and Tarjan's Analysis

Huffman codes and greedy algorithm

Dynamic programming: Weight-independent sets in path graphs, optimal substructure, O(n) algorithm, reconstruction algorithm

Knapsack problem and a dynamic programming algorithm

Sequence alignment's optimal substructure and a dynamic programming algorithm

Optimal binary search trees and a dynamic programming algorithm








Guiding principles for reasoning about algorithms and how to design fast algorithms:
1. Worst-case analysis - upper bounds are based on the worst case
-> Compare to average-case analysis - average runtime as opposed to longest runtime, or benchmark analysis - as opposed to mean runtime, typical-use runtime
-> Worst-case analysis is usually easiest to determine, and most generally applicable

2. Highest-order terms - don't worry about lower-order terms
-> Much easier to determine highest-order term, constants and lower-order terms depend on programming language, compiler, etc.
-> Typically lose very little predictive power

3. Asymptotic analysis - kind of the same thing as 2; asymptotically only the highest-order term matters, and asymptotically lower-order runtimes will be more efficient regardless of coefficient size

Ultimate goal: Linear runtime or as close to it as possible



Guiding principles for dynamic programming:
1. To apply dynamic programming, one must have:
-> Optimal substructure - the optimal solution to a problem must be achievable by recursively considering optimal solutions to overlapping subproblems. 
-> Overlapping subproblems - When subproblems do not overlap, one ought instead divide and conquer. Dynamic programming is effective when solutions cannot be found without recursion
-> A lot of CS problems can be formulated as directed graphs of subproblems, and a dynamic programming problem has the key feature of not being a tree structure
2. Consider each approach:
-> Top-down: Formulate the big problem recursively using the solution to subproblems, store solutions to subproblems in a table, and when a different subproblem requires a solution, have it check the table to see if the solution has been found
-> Bottom-up: Formulate the big problem recursively using the solution to smallest subproblems, solve subproblems first and use their solutions to build up solutions to bigger subproblems
3. Knowing how to identify subproblems is most of the battle, number of subproblems should be "small" (mindful ability to recursively apply solutions to subproblems to larger ones)



Overarching algorithm design paradigms: 
1. Divide and conquer - Break a big problem into big independent subproblems, each of which can be solved recursively, then assemble into big solution
-> Canonical example: Mergesort
2. Randomized algorithms - use properties of expectation and order-from-randomness to deduce more efficient solutions with high likelihood of correctness
-> Examples: Quicksort and random pivot elements for hash functions
3. Greedy algorithms - iteratively make myopic decisions
-> Example: Dijkstra's shortest path algorithm
4. Dynamic programming - Break a big problem into big dependent subproblems, and use recursion to build up a solution

Guiding principles for greedy algorithms:
- Such algorithms should be myopic in that each decision is processed once, is irrevocable, and looks only at immediately relevant information
- Tend to be easy to analyze runtime, but hard to prove correct (many greedy algorithms are not correct and don't generalize well)













For algorthmic trading:
Adaptive Shortfall, Basket Trading, Bollinger Band

ML is also valuable...


Machine learning notes:

A lot of ML is recognizing correlations. Often there's n independent variables and 1 dependent variable. This means a ML algorithm chooses an optimized linear functional in the space of linear functionals on R^n. By duality, this space is parameterized by R^n (or Z^n as the case may be depending on the data type). 

Main techniques:
Regression and Estimation (likely to be most relevant)
Classification (could also be relevant for classifying if a security will rise or fall)
Clustering (unlikely to be as relevant)
Associations (correlations)
Anomaly detection (likely to be more relevant nowadays than in past with the rise of caution about black swan events) 
Sequence mining (predicting future events is very valuable in finance)








Big-O Notation:
f(x) = O(g(x)) iff there is an n0 > 0, c > 0 st for all n > n0, f(n) <= cg(x)
(f is bounded above by g)

f(x) = o(g(x)) iff for any e > 0, there is an n0 > 0 st for all n > n0, f(n) <= eg(x)
(f is dominated by g asymptotically)

f(x) = Omega(g(x)) iff g(x) = O(f(x))
(f is bounded below by g)

f(x) = Theta(g(x)) iff f(x) = Omega(g(x)) and f(x) = O(g(x))
(f is of the same order as g)

Topics learned:
karatsuba multiplication
merge sort
big-oh notation
big omega and theta

O(n log n) algorithm for counting inversions

Quicksort and partitioning around a pivot
Heaps



